{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Building a Retrieval-Augmented Generation (RAG) System with LangChain"
      ],
      "metadata": {
        "id": "RsNt2qB0DWHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**"
      ],
      "metadata": {
        "id": "LQjPk9sCEFnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval-Augmented Generation (RAG) is an advanced AI approach that combines retrieval mechanisms with generative AI models. It enhances the accuracy and relevance of AI-generated responses by incorporating external data sources into the response generation process. In this blog, we will walk through the implementation of a RAG-based system using LangChain and OpenAIâ€™s GPT models."
      ],
      "metadata": {
        "id": "0D9Q1KJDDgoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-requisite**"
      ],
      "metadata": {
        "id": "2SprCCrODids"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving into the implementation, ensure you have the following dependencies installed:"
      ],
      "metadata": {
        "id": "ePkc0dyREby9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain langchain-community langchain-openai faiss-cpu"
      ],
      "metadata": {
        "id": "VH71BxUnEf48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, store your OpenAI API key and Langchain API Key in a .env file for secure access."
      ],
      "metadata": {
        "id": "MKqojRmREi8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up Environment Variables**"
      ],
      "metadata": {
        "id": "QUyxavVEEqmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
      ],
      "metadata": {
        "id": "4jE44i7pEtIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up the LLM**"
      ],
      "metadata": {
        "id": "0WjRDeZhE0dS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first initialize an OpenAI-based language model:"
      ],
      "metadata": {
        "id": "fjKYiqHSE55B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"o1-mini\")\n",
        "print(llm)"
      ],
      "metadata": {
        "id": "SZXh0hDIE3gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now invoke this model to generate responses:"
      ],
      "metadata": {
        "id": "QNrSeFYJFF9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.invoke(\"What is agentic AI?\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "WXseJrN3FG4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Chat Prompt**"
      ],
      "metadata": {
        "id": "gvCT2uyoFK60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make interactions more structured, we define a prompt template:"
      ],
      "metadata": {
        "id": "eCRN497vFSvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an expert AI Engineer. Provide an answer based on the question.\"),\n",
        "        (\"user\", \"{input}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "cbUkM0vZFThk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This template ensures that the AI model adheres to a specific role while generating responses. Next, we create a chain that connects the prompt and the language model:"
      ],
      "metadata": {
        "id": "-Np33zb7Fl0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "chain = prompt | llm\n",
        "response = chain.invoke({\"input\": \"Can you tell me about Langsmith?\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "HUEGDXeOFmkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing RAG**"
      ],
      "metadata": {
        "id": "skztunFIFzf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG combines retrieval mechanisms with generative AI to improve response accuracy. We begin by loading external documents:"
      ],
      "metadata": {
        "id": "cXASgb2oF3Wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://python.langchain.com/docs/tutorials/llm_chain/\")\n",
        "document = loader.load()"
      ],
      "metadata": {
        "id": "AZq-b-m_F1Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting the Document into Chunks**"
      ],
      "metadata": {
        "id": "Nlj0irhvGCh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To efficiently process large documents, we split them into manageable chunks:"
      ],
      "metadata": {
        "id": "otGbW02KGEyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "documents = text_splitter.split_documents(document)"
      ],
      "metadata": {
        "id": "zrD8HcfHGIpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Vector Store**"
      ],
      "metadata": {
        "id": "9bgP6hFBGKj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then create vector embeddings for the document chunks:"
      ],
      "metadata": {
        "id": "Lj3m-KreGOJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "1Y4uuWKUGP7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing a Retriever**"
      ],
      "metadata": {
        "id": "tr359ukyGTQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A retriever fetches relevant document chunks based on a user query:"
      ],
      "metadata": {
        "id": "LJMlTj1UGWTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "bjpjAd63GX9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the Retrieval Chain**"
      ],
      "metadata": {
        "id": "rG8XpBkKGatk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We integrate the retriever into our chain:"
      ],
      "metadata": {
        "id": "NWqFbXfvGjO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retrieval_chain = create_retrieval_chain(retriever, chain)"
      ],
      "metadata": {
        "id": "fJh6114zGiMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running the RAG Pipeline**"
      ],
      "metadata": {
        "id": "H_jqdZ5hG0Pe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can input a query and retrieve an answer:"
      ],
      "metadata": {
        "id": "kFgJLZDdG3Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = retrieval_chain.invoke({\"input\": \"Note that ChatModels receive message objects as input\"})\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "id": "YvNjji4bG4yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**"
      ],
      "metadata": {
        "id": "EDQ2sFn8G7IC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By implementing this RAG-based system, we enhance the response quality of LLMs by integrating document retrieval with generative AI. This approach is particularly useful for applications that require accurate, context-aware responses, such as customer support chatbots, research assistants, and AI-driven search engines."
      ],
      "metadata": {
        "id": "W_7vxmywG9sI"
      }
    }
  ]
}
